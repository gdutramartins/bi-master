{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Podemos ter uma segunda avaliação com peso de acordo com os trendings e seus direcionamentos para sentimento... ele pode aumentar ou diminuir o peso do sentimento...\n",
    "* Vamos usar comitês para tentar melhorar?\n",
    "* Trending tentaremos obter palavras dos ngrams? E incluí-las? Além do tratamento de trending e sentimento\n",
    "* We highly recommend the twokenize tokenizer from Allen Ritter’s work (https://github.com/aritter/twitter_nlp/blob/master/python/twokenize.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re,string,unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv1D,MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from wordsegment import load, segment\n",
    "\n",
    "import contractions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lista_stopwords = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "lista_stopwords.update(punctuation)\n",
    "\n",
    "#word segmentation\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARQUIVO_GLOVE_100 = 'glove.6B.100d.txt'\n",
    "ARQUIVO_GLOVE_100_TWITTER = 'glove.twitter.27B.100d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilitários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o arquivo de Embeddings\n",
    "def read_embedding(file_name):\n",
    "    with open(file_name,'r', encoding=\"utf8\") as f:\n",
    "        word_vocab = {} \n",
    "        word2vector = []\n",
    "        pos = 0\n",
    "        for line in f:\n",
    "            line_ = line.strip() \n",
    "            words_vec = line_.split()\n",
    "            \n",
    "            if (len(words_vec) == 101):\n",
    "                word_vocab[words_vec[0]] = pos\n",
    "                word2vector.append(np.array(words_vec[1:],dtype=float)) \n",
    "                pos+=1\n",
    "            \n",
    "    word2vector = np.stack(word2vector)\n",
    "    print(\"Total Words in DataSet:\",len(word_vocab))\n",
    "    return word_vocab,word2vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpa_texto(texto, use_hashtag_embedding):\n",
    "    # link\n",
    "    texto = re.sub('(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '<url>', texto)\n",
    "    # numero string com números não estamos retirando inicialmente (COVID-19)\n",
    "    texto = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', '<number>', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    \n",
    "    #texto = re.sub(r'\\b([A-Z]+)\\b', '<allcaps> \\g<1>', texto)\n",
    "    \n",
    "    if use_hashtag_embedding:\n",
    "        texto = re.sub(r'#([\\w]+)', '<hashtag> \\g<1>', texto)\n",
    "    else:\n",
    "        texto = re.sub(r'#', '', texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labels. Pemite agrupar negativos e positivos\n",
    "def get_labels_dict(agrupar_labels):\n",
    "    if not agrupar_labels:\n",
    "        return {'Extremely Negative': -2 , 'Negative': -1, 'Neutral': 0, 'Positive': 1, 'Extremely Positive': 2}\n",
    "    else:\n",
    "        return {'Negative': -1 ,'Neutral': 0, 'Positive': 1}    \n",
    "    \n",
    "    \n",
    "# Converter labels da representação textual para one hot encode\n",
    "def convert_labels_to_hot_encode(labels, labels_dict, agrupar_labels):\n",
    "    if agrupar_labels:\n",
    "        labels_conv =[labels_dict[label.split()[-1]] for label in labels]\n",
    "    else:\n",
    "        labels_conv =[labels_dict[label] for label in labels]\n",
    "    df_dummified = pd.get_dummies(labels_conv)\n",
    "    \n",
    "    columns = df_dummified.columns\n",
    "    print(columns)\n",
    "    \n",
    "    return df_dummified.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotina Pré-Processamento (Representação Vetorial/Contexto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separa tokens por representação vetorial\n",
    "def split_tokens_word_vec(texto, vocab, use_twitter_tokenizer, min_length_word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    texto = contractions.fix(texto) \n",
    "        \n",
    "    lista_tokens_inicial = []\n",
    "    if use_twitter_tokenizer:\n",
    "        t_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "        lista_tokens_inicial = t_tokenizer.tokenize(texto)\n",
    "    else:\n",
    "        lista_tokens_inicial = word_tokenize(texto)\n",
    "    \n",
    "    tokens_encontrados = []\n",
    "    tokens_nao_encontrados = []\n",
    "    \n",
    "    for token in lista_tokens_inicial:\n",
    "        add_token = True\n",
    "        \n",
    "        #if token in lista_stopwords:\n",
    "        #    add_token = False\n",
    "        \n",
    "        if len(token) < min_length_word:\n",
    "            add_token = False\n",
    "        \n",
    "        if add_token:\n",
    "            if token.lower() in vocab:\n",
    "                tokens_encontrados.append(token.lower())\n",
    "            else:\n",
    "                sinonimo = get_sinonimo(token.lower())\n",
    "                if sinonimo != None:\n",
    "                    tokens_encontrados.append(sinonimo)\n",
    "                else:\n",
    "                    tokens_salvos = trata_token_nao_encontrado(token, vocab)\n",
    "                    if len(tokens_salvos) > 0:\n",
    "                        tokens_encontrados.extend(tokens_salvos)\n",
    "                    else:\n",
    "                        #tokens_encontrados.append('<unknown>')\n",
    "                        tokens_nao_encontrados.append(token)\n",
    "    return tokens_encontrados, tokens_nao_encontrados\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trata_token_nao_encontrado(token, vocab):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_salvos = []\n",
    "    \n",
    "    token_lemmatizado = lemmatizer.lemmatize(token)\n",
    "    \n",
    "    if token != token_lemmatizado:\n",
    "        if token_lemmatizado in vocab:\n",
    "            return [token_lemmatizado]\n",
    "    \n",
    "    tokens_word_segmentation = segment(token)\n",
    "    \n",
    "    for subtoken in tokens_word_segmentation:\n",
    "        if subtoken in vocab:\n",
    "            tokens_salvos.append(subtoken)\n",
    "        else:\n",
    "            subtoken_lemmatizado = lemmatizer.lemmatize(subtoken)\n",
    "            if (subtoken_lemmatizado in vocab):\n",
    "                tokens_salvos.append(subtoken_lemmatizado)\n",
    "            else:\n",
    "                sinonimo = get_sinonimo(subtoken)\n",
    "                if sinonimo != None:\n",
    "                    tokens_salvos.append(sinonimo)                \n",
    "    return tokens_salvos\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinonimo(str):\n",
    "    dict_sinonimos = {'covid': 'coronavirus', 'chloroquine':'medicine'}\n",
    "    \n",
    "    sinonimo = None\n",
    "    \n",
    "    if str in dict_sinonimos:\n",
    "        sinonimo = dict_sinonimos[str]\n",
    "    return sinonimo\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_tokens_word_vec(lista_tweet, vocab, use_twitter_tokenizer,  min_length_word, use_hashtag_embedding):\n",
    "    feature_tokens = []\n",
    "    feature_tokens_verificar =[]\n",
    "    for tweet in lista_tweet:\n",
    "        texto_limpo = limpa_texto(tweet, use_hashtag_embedding)\n",
    "        tokens_add, tokens_verify = split_tokens_word_vec(texto_limpo, \n",
    "                                                          vocab,\n",
    "                                                          use_twitter_tokenizer=use_twitter_tokenizer, \n",
    "                                                          min_length_word=min_length_word)\n",
    "        feature_tokens.append(np.asarray(tokens_add))\n",
    "        feature_tokens_verificar.append(tokens_verify)\n",
    "    \n",
    "    \n",
    "    feature_tokens = np.asarray(feature_tokens)\n",
    "    feature_tokens_verificar = np.asarray(feature_tokens_verificar)\n",
    "    \n",
    "    return feature_tokens, feature_tokens_verificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processamento_word_vec(dados, \n",
    "                               agrupar_labels=False, \n",
    "                               glove_twitter=True, \n",
    "                               min_length_word=0, \n",
    "                               convert_labels_hot_encode=True, \n",
    "                               use_twitter_tokenizer=True,\n",
    "                               use_hashtag_embedding=False):\n",
    "    arquivo_glove = ARQUIVO_GLOVE_100_TWITTER\n",
    "    if not glove_twitter :\n",
    "        arquivo_glove = ARQUIVO_GLOVE_100\n",
    "    \n",
    "    lista_tweet = dados.OriginalTweet\n",
    "    labels_str = dados.Sentiment\n",
    "    lista_sentimentos = dados.Sentiment.unique()\n",
    "    \n",
    "    vocab, embedding = read_embedding(arquivo_glove)\n",
    "    \n",
    "    labels_dict = get_labels_dict(agrupar_labels)\n",
    "    if convert_labels_hot_encode:\n",
    "        labels = convert_labels_to_hot_encode(labels_str, labels_dict, agrupar_labels)\n",
    "    else:\n",
    "        if (agrupar_labels):\n",
    "            labels = [labels_dict[label.split()[-1]] for label in labels_str]\n",
    "        else:\n",
    "            labels = [labels_dict[label] for label in labels_str]\n",
    "        labels = np.asarray(labels)\n",
    "    \n",
    "    feature_tokens, feature_tokens_verificar = carrega_tokens_word_vec(lista_tweet, \n",
    "                                                                       vocab, \n",
    "                                                                       use_twitter_tokenizer=use_twitter_tokenizer, \n",
    "                                                                       min_length_word=min_length_word,\n",
    "                                                                      use_hashtag_embedding=use_hashtag_embedding)\n",
    "        \n",
    "    return (vocab, embedding), (feature_tokens, feature_tokens_verificar), labels, labels_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "def pre_processamento_word_vec_test(dados, \n",
    "                                    vocab,\n",
    "                                    labels_dict,\n",
    "                                   agrupar_labels=False, \n",
    "                                   glove_twitter=True, \n",
    "                                   min_length_word=0, \n",
    "                                   use_twitter_tokenizer=True, \n",
    "                                   use_hashtag_embedding=False):\n",
    "    \n",
    "    arquivo_glove = ARQUIVO_GLOVE_100_TWITTER\n",
    "    if not glove_twitter :\n",
    "        arquivo_glove = ARQUIVO_GLOVE_100\n",
    "    \n",
    "    lista_tweet = dados.OriginalTweet\n",
    "    labels_str = dados.Sentiment\n",
    "    \n",
    "    \n",
    "    labels = convert_labels_to_hot_encode(labels_str, labels_dict, agrupar_labels)\n",
    "    \n",
    "    feature_tokens, _ = carrega_tokens_word_vec(lista_tweet, \n",
    "                                                vocab, \n",
    "                                                use_twitter_tokenizer=use_twitter_tokenizer, \n",
    "                                                min_length_word=min_length_word,\n",
    "                                                use_hashtag_embedding=use_hashtag_embedding)\n",
    "        \n",
    "    return feature_tokens, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dados))\n",
    "print(dados.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.value_counts(['Location','Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados[['OriginalTweet','Sentiment']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud\n",
    "def monta_wordcloud(lista_topicos, texto_ou_frequencia, generate_from_text=True):\n",
    "    num_topicos = len(lista_topicos)\n",
    "    ncols = 2\n",
    "    col = 0\n",
    "    lin  = 1\n",
    "\n",
    "    fig, ax = plt.subplots(1, ncols, figsize=(30, 20))\n",
    "\n",
    "    for topico in lista_topicos:\n",
    "        if generate_from_text:\n",
    "            wordcloud = WordCloud(background_color=\"black\",collocations=False,\n",
    "                              colormap=\"Oranges_r\",\n",
    "                              width = 1000,\n",
    "                              height = 1000,\n",
    "                              max_font_size=1000,\n",
    "                              max_words=30\n",
    "                             ).generate(texto_ou_frequencia[topico])\n",
    "        else:\n",
    "            wordcloud = WordCloud(background_color=\"black\",collocations=False,\n",
    "                              colormap=\"Oranges_r\",\n",
    "                              width = 1000,\n",
    "                              height = 1000,\n",
    "                              max_font_size=1000,\n",
    "                              max_words=30\n",
    "                             ).generate_from_frequencies(texto_ou_frequencia[topico])\n",
    "    \n",
    "        ax[col].imshow(wordcloud) \n",
    "        ax[col].set_title(f\"{topico}\")\n",
    "        # No axis details\n",
    "        ax[col].axis(\"off\");\n",
    "        col = col + 1\n",
    "        if col >= ncols:\n",
    "            plt.show()\n",
    "            lin = lin + 1\n",
    "            col = 0\n",
    "            ax_cols =  ncols\n",
    "            fig, ax = plt.subplots(1, ax_cols, figsize=(30, 20))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv('Corona_NLP_train.csv', engine=\"python\")\n",
    "(_, _), (feature_tokens, feature_tokens_verificar) , labels, labels_dict = pre_processamento_word_vec(dados, glove_twitter=True, convert_labels_hot_encode=False,min_length_word=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud por Sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sentimento = {}\n",
    "for sentimento in labels_dict:\n",
    "    text_sentimento[sentimento] = \" \".join([token for index, f_token in enumerate(feature_tokens) if labels[index] == labels_dict[sentimento] for token in f_token ])  \n",
    "monta_wordcloud(labels_dict, text_sentimento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud por Trending/Sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_count_trends = Counter()\n",
    "_count_trends.update([token for index, lista_token in enumerate(X_tokens + X_tokens_verificar) if y[index] == sentimento for token in lista_token if re.match(r'#\\w+',token) ])\n",
    "monta_wordcloud(lista_sentimentos, _count_trends, generate_from_text=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud palavras não Encontradas (Sem Trending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sentimento = {}\n",
    "for sentimento in labels_dict:\n",
    "    text_sentimento[sentimento] = \" \".join([token for index, f_token in enumerate(feature_tokens_verificar) if labels[index] == labels_dict[sentimento] for token in f_token ])  \n",
    "monta_wordcloud(labels_dict, text_sentimento)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_lstm_pure(weight_matrix, dim_embedding, max_words_length, num_labels=5):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(weight_matrix), dim_embedding, weights=[weight_matrix], input_length=max_words_length, trainable=False))\n",
    "    # model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    #model.add(Dense(512, activation='relu'))\n",
    "    #model.add(Dropout(0.50))\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    # Adam Optimiser\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_lstm_pure_bi(weight_matrix, dim_embedding, max_words_length, num_labels=5):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(weight_matrix), dim_embedding, weights=[weight_matrix], input_length=max_words_length, trainable=False))\n",
    "    # model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    #model.add(Dense(512, activation='relu'))\n",
    "    #model.add(Dropout(0.50))\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    # Adam Optimiser\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_lstm_dense(weight_matrix, dim_embedding, max_words_length, num_labels=5):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(weight_matrix), dim_embedding, weights=[weight_matrix], input_length=max_words_length, trainable=False))\n",
    "    # model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    # Adam Optimiser\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_model_lstm_bi_dense(weight_matrix, dim_embedding, max_words_length, num_labels=5):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(weight_matrix), dim_embedding, weights=[weight_matrix], input_length=max_words_length, trainable=False))\n",
    "    # model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    # Adam Optimiser\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_model_lstm_conv(weight_matrix, dim_embedding, max_words_length, num_labels=5):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(weight_matrix), dim_embedding, weights=[weight_matrix], input_length=max_words_length, trainable=False))\n",
    "    # model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    # Adam Optimiser\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def crate_model_double_lstm(weight_matrix, dim_embedding, max_words_length, num_labels=5):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(weight_matrix), dim_embedding, weights=[weight_matrix], input_length=max_words_length, trainable=False))\n",
    "    model.add(Bidirectional(LSTM(120, return_sequences=True, recurrent_dropout=0.5)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(120, recurrent_dropout=0.5)))\n",
    "    model.add(Dense(60, activation='relu')) #60\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    # Adam Optimiser \n",
    "    # testar Adam(lr=0.01, decay=0.001)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(lr=0.01, decay=0.001), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter tokens para a representação word vec\n",
    "def convert_feature_tokens_to_word_vec(feature_tokens,  vocab, dimension_size=100, max_word_length=50):\n",
    "    feature_word_vec = np.zeros((len(feature_tokens), max_word_length), dtype='int32')\n",
    "    \n",
    "    for c_feature, f_token in enumerate(feature_tokens):\n",
    "        for c_token,token in enumerate(f_token):\n",
    "            if(c_token < max_word_length):\n",
    "                feature_word_vec[c_feature,c_token] = vocab[token]\n",
    "        \n",
    "    return feature_word_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words in DataSet: 1193513\n",
      "Int64Index([-1, 0, 1], dtype='int64')\n",
      "Int64Index([-1, 0, 1], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "max_word_length=50\n",
    "use_hashtag_embedding=True\n",
    "\n",
    "\n",
    "dados = pd.read_csv('Corona_NLP_train.csv', engine=\"python\")\n",
    "dados_test = pd.read_csv('Corona_NLP_test.csv', engine=\"python\")\n",
    "\n",
    "(vocab, embedding), (feature_tokens, _), labels, labels_dict = pre_processamento_word_vec(dados, \n",
    "                                                                                          agrupar_labels=True,\n",
    "                                                                                          glove_twitter=True, \n",
    "                                                                                          convert_labels_hot_encode=True,\n",
    "                                                                                          min_length_word=1,\n",
    "                                                                                          use_twitter_tokenizer=True,\n",
    "                                                                                          use_hashtag_embedding=use_hashtag_embedding)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(feature_tokens, labels,  test_size=0.2, random_state=0)\n",
    "\n",
    "X_train = convert_feature_tokens_to_word_vec(X_train, \n",
    "                                             vocab,\n",
    "                                             max_word_length=max_word_length)\n",
    "X_val = convert_feature_tokens_to_word_vec(X_val, \n",
    "                                           vocab, \n",
    "                                           max_word_length=max_word_length)\n",
    "\n",
    "X_test, y_test = pre_processamento_word_vec_test(dados_test,\n",
    "                                          vocab,\n",
    "                                          labels_dict,\n",
    "                                          agrupar_labels=True,\n",
    "                                          glove_twitter=True,\n",
    "                                          min_length_word=1,\n",
    "                                          use_twitter_tokenizer=True,\n",
    "                                          use_hashtag_embedding=use_hashtag_embedding)\n",
    "\n",
    "X_test = convert_feature_tokens_to_word_vec(X_test, \n",
    "                                           vocab, \n",
    "                                           max_word_length=max_word_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pure = create_model_lstm_pure(embedding, 100, max_word_length, len(labels_dict))\n",
    "history_pure = model_pure.fit(X_train, y_train, validation_data=(X_val, y_val),  batch_size=32, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pure = model_pure.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(evaluate_pure[0],evaluate_pure[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pure_bi = create_model_lstm_pure_bi(embedding, 100, max_word_length, len(labels_dict))\n",
    "history_pure_bi = model_pure_bi.fit(X_train, y_train, validation_data=(X_val, y_val),  batch_size=32, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pure_bi = model_pure_bi.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(evaluate_pure_bi[0],evaluate_pure_bi[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense = create_model_lstm_dense(embedding, 100, max_word_length, len(labels_dict))\n",
    "history_dense = model_dense.fit(X_train, y_train, validation_data=(X_val, y_val),  batch_size=32, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dense = model_dense.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(evaluate_dense[0],evaluate_dense[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi_dense = create_model_lstm_bi_dense(embedding, 100, max_word_length, len(labels_dict))\n",
    "history_bi_dense = model_bi_dense.fit(X_train, y_train, validation_data=(X_val, y_val),  batch_size=32, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_bi_dense = model_bi_dense.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(evaluate_bi_dense[0],evaluate_bi_dense[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_conv = create_model_lstm_conv(embedding, 100, max_word_length, len(labels_dict))\n",
    "filepath=\"weights_best_lstm_cnn.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "history_bi_dense = model_lstm_conv.fit(X_train, y_train, validation_data=(X_val, y_val),  batch_size=32, epochs=20, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.8576 - accuracy: 0.6016\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.72449, saving model to weights_best_double_lstm.hdf5\n",
      "258/258 [==============================] - 379s 1s/step - loss: 0.8576 - accuracy: 0.6016 - val_loss: 0.6634 - val_accuracy: 0.7245\n",
      "Epoch 2/12\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.6033 - accuracy: 0.7570\n",
      "Epoch 00002: val_accuracy improved from 0.72449 to 0.78790, saving model to weights_best_double_lstm.hdf5\n",
      "258/258 [==============================] - 389s 2s/step - loss: 0.6033 - accuracy: 0.7570 - val_loss: 0.5446 - val_accuracy: 0.7879\n",
      "Epoch 3/12\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.5021 - accuracy: 0.8087\n",
      "Epoch 00003: val_accuracy improved from 0.78790 to 0.83601, saving model to weights_best_double_lstm.hdf5\n",
      "258/258 [==============================] - 380s 1s/step - loss: 0.5021 - accuracy: 0.8087 - val_loss: 0.4573 - val_accuracy: 0.8360\n",
      "Epoch 4/12\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.4217 - accuracy: 0.8452\n",
      "Epoch 00004: val_accuracy improved from 0.83601 to 0.84973, saving model to weights_best_double_lstm.hdf5\n",
      "258/258 [==============================] - 383s 1s/step - loss: 0.4217 - accuracy: 0.8452 - val_loss: 0.4211 - val_accuracy: 0.8497\n",
      "Epoch 5/12\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.3734 - accuracy: 0.8660\n",
      "Epoch 00005: val_accuracy improved from 0.84973 to 0.85569, saving model to weights_best_double_lstm.hdf5\n",
      "258/258 [==============================] - 386s 1s/step - loss: 0.3734 - accuracy: 0.8660 - val_loss: 0.4042 - val_accuracy: 0.8557\n",
      "Epoch 6/12\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.8817\n",
      "Epoch 00006: val_accuracy improved from 0.85569 to 0.85787, saving model to weights_best_double_lstm.hdf5\n",
      "258/258 [==============================] - 386s 1s/step - loss: 0.3345 - accuracy: 0.8817 - val_loss: 0.4118 - val_accuracy: 0.8579\n",
      "Epoch 7/12\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.3042 - accuracy: 0.8928\n",
      "Epoch 00007: val_accuracy improved from 0.85787 to 0.86552, saving model to weights_best_double_lstm.hdf5\n",
      "258/258 [==============================] - 393s 2s/step - loss: 0.3042 - accuracy: 0.8928 - val_loss: 0.3967 - val_accuracy: 0.8655\n",
      "Epoch 8/12\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2766 - accuracy: 0.9029\n",
      "Epoch 00008: val_accuracy improved from 0.86552 to 0.87051, saving model to weights_best_double_lstm.hdf5\n",
      "258/258 [==============================] - 392s 2s/step - loss: 0.2766 - accuracy: 0.9029 - val_loss: 0.3867 - val_accuracy: 0.8705\n",
      "Epoch 9/12\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.9073\n",
      "Epoch 00009: val_accuracy improved from 0.87051 to 0.87488, saving model to weights_best_double_lstm.hdf5\n",
      "258/258 [==============================] - 401s 2s/step - loss: 0.2623 - accuracy: 0.9073 - val_loss: 0.3752 - val_accuracy: 0.8749\n",
      "Epoch 10/12\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2337 - accuracy: 0.9186\n",
      "Epoch 00010: val_accuracy did not improve from 0.87488\n",
      "258/258 [==============================] - 387s 1s/step - loss: 0.2337 - accuracy: 0.9186 - val_loss: 0.3993 - val_accuracy: 0.8724\n",
      "Epoch 11/12\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.9208\n",
      "Epoch 00011: val_accuracy improved from 0.87488 to 0.87658, saving model to weights_best_double_lstm.hdf5\n",
      "258/258 [==============================] - 391s 2s/step - loss: 0.2211 - accuracy: 0.9208 - val_loss: 0.4053 - val_accuracy: 0.8766\n",
      "Epoch 12/12\n",
      "258/258 [==============================] - ETA: 0s - loss: 0.2028 - accuracy: 0.9289\n",
      "Epoch 00012: val_accuracy did not improve from 0.87658\n",
      "258/258 [==============================] - 385s 1s/step - loss: 0.2028 - accuracy: 0.9289 - val_loss: 0.4273 - val_accuracy: 0.8733\n"
     ]
    }
   ],
   "source": [
    "model_dobule_lstm = crate_model_double_lstm(embedding, 100, max_word_length, len(labels_dict))\n",
    "\n",
    "filepath=\"weights_best_double_lstm.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history_double_lstm = model_dobule_lstm.fit(X_train, y_train, validation_data=(X_val, y_val),  batch_size=128, epochs=12, callbacks=callbacks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 22s 182ms/step - loss: 0.4506 - accuracy: 0.8573s -\n",
      "Test set\n",
      "  Loss: 0.451\n",
      "  Accuracy: 0.857\n"
     ]
    }
   ],
   "source": [
    "filepath=\"weights_best_double_lstm.hdf5\"\n",
    "model_dobule_lstm.load_weights(filepath)\n",
    "evaluate_double_lstm = model_dobule_lstm.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(evaluate_double_lstm[0],evaluate_double_lstm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
