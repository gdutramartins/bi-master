{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "specified-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "peripheral-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_DRUG_PROTEIN = 'DRUG-PROTEIN'\n",
    "LABEL_CHEMICAL = 'CHEMICAL'\n",
    "LABEL_DISEASE = 'DISEASE'\n",
    "LABEL_SPECIES = 'SPECIES'\n",
    "\n",
    "LABEL_LIST = [LABEL_DRUG_PROTEIN,\n",
    "              LABEL_CHEMICAL,\n",
    "              LABEL_DISEASE,    \n",
    "              LABEL_SPECIES]\n",
    "\n",
    "LABEL_TO_DIR = {\n",
    "    LABEL_DRUG_PROTEIN: ['BC2GM', 'JNLPBA'],\n",
    "    LABEL_CHEMICAL: ['BC4CHEMD','BC5CDR-chem'],\n",
    "    LABEL_DISEASE: ['BC5CDR-disease', 'NCBI-disease'],    \n",
    "    LABEL_SPECIES: ['linnaeus', 's800']\n",
    "}\n",
    "\n",
    "DATASET_PATH = 'NER-Data'\n",
    "\n",
    "DATASET_TYPE = [\"train_dev\", \"test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-functionality",
   "metadata": {},
   "source": [
    "## Converson para JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fatal-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "\n",
    "for dataset_type in DATASET_TYPE:\n",
    "    for label in LABEL_LIST:\n",
    "        sentenca = \"\"\n",
    "        entities = []\n",
    "        lista_ner = []\n",
    "        ini_entity_atual = -1\n",
    "        pos_atual = 0\n",
    "        entidade_atual = \"\"\n",
    "        \n",
    "        for dir_dataset in LABEL_TO_DIR[label]:\n",
    "            dataset_ner_file = os.path.join(DATASET_PATH, dir_dataset, dataset_type + \".tsv\")\n",
    "            with open(dataset_ner_file) as f_ner:\n",
    "                for linha in f_ner:\n",
    "                    if len(entidade_atual) > 0 and (\"\\tO\" in linha or \"\\tB\" in linha or linha == \"\\n\"):\n",
    "                        entities.append({\"entidade\":entidade_atual, \n",
    "                                             \"start\":ini_entity_atual, \n",
    "                                             \"end\": ini_entity_atual + len(entidade_atual),\n",
    "                                             \"label\": label\n",
    "                                            })\n",
    "                        entidade_atual = \"\"\n",
    "                        ini_entity_atual = -1\n",
    "                        \n",
    "                    if linha != \"\\n\":\n",
    "                        if (pos_atual != 0):\n",
    "                            sentenca += \" \"\n",
    "                            pos_atual += 1\n",
    "                        if len(entidade_atual) > 0:\n",
    "                            entidade_atual += \" \"\n",
    "                        \n",
    "                        if (\"\\tO\" in linha):\n",
    "                            linha_tratada = linha.replace(\"\\tO\",\"\").replace(\"\\n\", \"\")                            \n",
    "                        elif(\"\\tB\" in linha):\n",
    "                            ini_entity_atual = pos_atual\n",
    "                            linha_tratada = linha.replace(\"\\tB\",\"\").replace(\"\\n\", \"\")\n",
    "                            entidade_atual = linha_tratada                \n",
    "                        elif(\"\\tI\" in linha):\n",
    "                            linha_tratada = linha.replace(\"\\tI\",\"\").replace(\"\\n\", \"\")\n",
    "                            entidade_atual += linha_tratada\n",
    "\n",
    "                        pos_atual += len(linha_tratada)\n",
    "                        sentenca = sentenca + linha_tratada\n",
    "                    else:\n",
    "                        lista_ner.append({\"texto\": sentenca, \"entities\": entities})\n",
    "                        sentenca = \"\"\n",
    "                        entities=[]\n",
    "                        pos_atual = 0\n",
    "\n",
    "\n",
    "        with open(os.path.join(DATASET_PATH, label + \"-\" + dataset_type + \".json\"), 'w') as json_file:            \n",
    "            json.dump(lista_ner, json_file)\n",
    "              \n",
    "        db = DocBin() # create a DocBin object\n",
    "        for ner in lista_ner:\n",
    "            doc = nlp.make_doc(ner['texto']) # create doc object from text\n",
    "            ents=[]\n",
    "            for entidade in ner['entities']:\n",
    "                span = doc.char_span(entidade['start'], entidade['end'], label=entidade['label'], alignment_mode=\"contract\")\n",
    "                if span is None:\n",
    "                    print (\"Span None\")\n",
    "                    print(ner['texto'])\n",
    "                    print(entidade)\n",
    "                else:\n",
    "                    ents.append(span)\n",
    "\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "        db.to_disk(os.path.join(DATASET_PATH, label + \"-\" + dataset_type + \".spacy\")) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
