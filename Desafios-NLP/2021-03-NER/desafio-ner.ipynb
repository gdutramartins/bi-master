{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "casual-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "agricultural-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_DRUG_PROTEIN = 'DRUG-PROTEIN'\n",
    "LABEL_CHEMICAL = 'CHEMICAL'\n",
    "LABEL_DISEASE = 'DISEASE'\n",
    "LABEL_SPECIES = 'SPECIES'\n",
    "\n",
    "LABEL_LIST = [LABEL_DRUG_PROTEIN,\n",
    "              LABEL_CHEMICAL,\n",
    "              LABEL_DISEASE,    \n",
    "              LABEL_SPECIES]\n",
    "\n",
    "LABEL_TO_DIR = {\n",
    "    LABEL_DRUG_PROTEIN: ['BC2GM', 'JNLPBA'],\n",
    "    LABEL_CHEMICAL: ['BC4CHEMD','BC5CDR-chem'],\n",
    "    LABEL_DISEASE: ['BC5CDR-disease', 'NCBI-disease'],    \n",
    "    LABEL_SPECIES: ['linnaeus', 's800']\n",
    "}\n",
    "\n",
    "DATASET_PATH = \"NER-Data\"\n",
    "NER_PATH = \"NER-Process\"\n",
    "\n",
    "TRAIN_DEV_DATASET = \"train_dev\"\n",
    "TRAIN_DATASET = \"train\"\n",
    "VALIDATE_DATASET = \"devel\"\n",
    "TEST_DATASET = \"test\"\n",
    "\n",
    "DATASET_TYPE = [TRAIN_DATASET, VALIDATE_DATASET, TEST_DATASET]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-sauce",
   "metadata": {},
   "source": [
    "## Converson para JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "micro-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "\n",
    "for dataset_type in DATASET_TYPE:\n",
    "    for label in LABEL_LIST:\n",
    "        sentenca = \"\"\n",
    "        entities = []\n",
    "        lista_ner = []\n",
    "        ini_entity_atual = -1\n",
    "        pos_atual = 0\n",
    "        entidade_atual = \"\"\n",
    "        \n",
    "        for dir_dataset in LABEL_TO_DIR[label]:\n",
    "            dataset_ner_file = os.path.join(DATASET_PATH, dir_dataset, dataset_type + \".tsv\")\n",
    "            with open(dataset_ner_file) as f_ner:\n",
    "                for linha in f_ner:\n",
    "                    if len(entidade_atual) > 0 and (\"\\tO\" in linha or \"\\tB\" in linha or linha == \"\\n\"):\n",
    "                        entities.append({\"entidade\":entidade_atual, \n",
    "                                             \"start\":ini_entity_atual, \n",
    "                                             \"end\": ini_entity_atual + len(entidade_atual),\n",
    "                                             \"label\": label\n",
    "                                            })\n",
    "                        entidade_atual = \"\"\n",
    "                        ini_entity_atual = -1\n",
    "                        \n",
    "                    if linha != \"\\n\":\n",
    "                        if (pos_atual != 0):\n",
    "                            sentenca += \" \"\n",
    "                            pos_atual += 1\n",
    "                        if len(entidade_atual) > 0:\n",
    "                            entidade_atual += \" \"\n",
    "                        \n",
    "                        if (\"\\tO\" in linha):\n",
    "                            linha_tratada = linha.replace(\"\\tO\",\"\").replace(\"\\n\", \"\")                            \n",
    "                        elif(\"\\tB\" in linha):\n",
    "                            ini_entity_atual = pos_atual\n",
    "                            linha_tratada = linha.replace(\"\\tB\",\"\").replace(\"\\n\", \"\")\n",
    "                            entidade_atual = linha_tratada                \n",
    "                        elif(\"\\tI\" in linha):\n",
    "                            linha_tratada = linha.replace(\"\\tI\",\"\").replace(\"\\n\", \"\")\n",
    "                            entidade_atual += linha_tratada\n",
    "\n",
    "                        pos_atual += len(linha_tratada)\n",
    "                        sentenca = sentenca + linha_tratada\n",
    "                    else:\n",
    "                        lista_ner.append({\"texto\": sentenca, \"entities\": entities})\n",
    "                        sentenca = \"\"\n",
    "                        entities=[]\n",
    "                        pos_atual = 0\n",
    "\n",
    "        path_label = os.path.join(NER_PATH, label)\n",
    "        file_json = os.path.join(path_label, label + \"-\" + dataset_type + \".json\")\n",
    "        Path(path_label).mkdir(parents=True, exist_ok=True)\n",
    "        with open(file_json, 'w') as json_file:            \n",
    "            json.dump(lista_ner, json_file)\n",
    "              \n",
    "        db = DocBin() # create a DocBin object\n",
    "        for ner in lista_ner:\n",
    "            doc = nlp.make_doc(ner['texto']) # create doc object from text\n",
    "            ents=[]\n",
    "            for entidade in ner['entities']:\n",
    "                span = doc.char_span(entidade['start'], entidade['end'], label=entidade['label'], alignment_mode=\"contract\")\n",
    "                if span is None:\n",
    "                    print (\"Span None\")\n",
    "                    print(ner['texto'])\n",
    "                    print(entidade)\n",
    "                else:\n",
    "                    ents.append(span)\n",
    "\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "        \n",
    "        file_spacy = os.path.join(path_label, label + \"-\" + dataset_type + \".spacy\")\n",
    "        db.to_disk(file_spacy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-avenue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "realistic-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(label, data_set_type=TRAIN_DATASET):\n",
    "    train_data = []\n",
    "    file = os.path.join(NER_PATH, label, label + \"-\" + data_set_type + \".json\")\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    for item in data:\n",
    "        texto = item[\"texto\"]\n",
    "        entities = []\n",
    "        for entidade in item[\"entities\"]:\n",
    "            info = (entidade[\"start\"], entidade[\"end\"], entidade[\"label\"])\n",
    "            entities.append(info)\n",
    "        train_data.append((texto,{\"entities\":entities}))\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "digital-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_evalation(data, nlp_model): \n",
    "    data_formated = []\n",
    "    for text, annotations in data:\n",
    "        doc = nlp_model.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        data_formated.append(example)\n",
    "    return data_formated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "talented-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spacy(train_data, validate_data, iterations, label):\n",
    "    ner_name = label + \"_ner\"\n",
    "    \n",
    "    nlp_train = spacy.blank(\"en\")\n",
    "    #nlp_train = spacy.load(\"en_core_web_md\")\n",
    "    \n",
    "    ner = nlp_train.add_pipe(\"ner\",name=ner_name)\n",
    "    ner.add_label(label)    \n",
    "    \n",
    "    best_ents_p = -1\n",
    "    best_ents_r = -1\n",
    "    best_ents_f = -1\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp_train.pipe_names if pipe != ner_name]\n",
    "    with nlp_train.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp_train.begin_training()\n",
    "        #optimizer = nlp_train.create_optimizer()\n",
    "        print(f\"{'#IT':5} | {'Loss':10} | {'Prec':10} | {'Recall':10} | {'F_Score':10} | {'Save Mod':10} \")\n",
    "        for itn in range(iterations):\n",
    "            linha = f\"{str(itn):5} | \"\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            batches = minibatch(train_data, size=512)\n",
    "            for batch in batches:\n",
    "                for text, annotations in batch:\n",
    "                    doc = nlp_train.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    nlp_train.update( [example],\n",
    "                        drop=0.2,  \n",
    "                        sgd=optimizer,\n",
    "                        losses=losses)\n",
    "            \n",
    "            validate_metrics = nlp_train.evaluate(format_data_for_evalation(validate_data, nlp_train))\n",
    "            linha += f\"{losses[list(losses)[0]]:08.2f} | {validate_metrics['ents_p']*100:05.2f} | {validate_metrics['ents_r']*100:05.2f} | {validate_metrics['ents_f']*100:05.2f} | \"\n",
    "            \n",
    "            if (validate_metrics[\"ents_p\"] > best_ents_p and validate_metrics[\"ents_f\"] > best_ents_f):\n",
    "                best_ents_r = validate_metrics[\"ents_r\"]\n",
    "                best_ents_f = validate_metrics[\"ents_f\"]\n",
    "                best_ents_p = validate_metrics[\"ents_p\"]\n",
    "                path = os.path.join(NER_PATH, label, \"best_model\")\n",
    "                linha += \"S \\n\"\n",
    "                \n",
    "                Path(path).mkdir(parents=True, exist_ok=True)\n",
    "                nlp_train.to_disk(path)                \n",
    "            else:\n",
    "                linha += \"N\"\n",
    "            print(linha)\n",
    "    return nlp_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "criminal-disclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#IT   | Loss       | Prec       | Recall     | F_Score    | Save Mod   \n",
      "\n",
      "0     | \n",
      "{'SPECIES_ner': 4382.3382432607} | 0.7147613762486127 | 0.5881278538812785 | 0.6452905811623246 | \n",
      "S \n",
      "\n",
      "1     | \n",
      "{'SPECIES_ner': 2864.2779926537396} | 0.6951735817104149 | 0.7497716894977169 | 0.7214411247803164 | \n",
      "N \n",
      "\n",
      "2     | \n",
      "{'SPECIES_ner': 2456.4296717148363} | 0.8433079434167573 | 0.7077625570776256 | 0.7696127110228401 | \n",
      "S \n",
      "\n",
      "3     | \n",
      "{'SPECIES_ner': 2099.9769765368787} | 0.8920086393088553 | 0.754337899543379 | 0.8174171202375062 | \n",
      "S \n",
      "\n",
      "4     | \n",
      "{'SPECIES_ner': 1899.7548692341943} | 0.8140703517587939 | 0.7397260273972602 | 0.7751196172248802 | \n",
      "N \n",
      "\n",
      "5     | \n",
      "{'SPECIES_ner': 1814.0921068344476} | 0.8736842105263158 | 0.6063926940639269 | 0.7159029649595687 | \n",
      "N \n",
      "\n",
      "6     | \n",
      "{'SPECIES_ner': 1838.4631381303398} | 0.8461538461538461 | 0.7132420091324201 | 0.7740336967294351 | \n",
      "N \n",
      "\n",
      "7     | \n",
      "{'SPECIES_ner': 1711.0829395561345} | 0.8929936305732484 | 0.6401826484018265 | 0.7457446808510639 | \n",
      "N \n",
      "\n",
      "8     | \n",
      "{'SPECIES_ner': 1748.9248377166193} | 0.8680042238648363 | 0.7506849315068493 | 0.8050930460333008 | \n",
      "N \n",
      "\n",
      "9     | \n",
      "{'SPECIES_ner': 1572.129750143433} | 0.9236276849642004 | 0.7068493150684931 | 0.8008277289187791 | \n",
      "N \n",
      "\n",
      "10    | \n",
      "{'SPECIES_ner': 1527.278945786975} | 0.9121706398996235 | 0.6639269406392694 | 0.7684989429175476 | \n",
      "N \n",
      "\n",
      "11    | \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-56d53608764a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLABEL_SPECIES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRAIN_DATASET\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvalidate_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLABEL_SPECIES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVALIDATE_DATASET\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_spacy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidate_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLABEL_SPECIES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-127-5bedb0aaa130>\u001b[0m in \u001b[0;36mtrain_spacy\u001b[1;34m(train_data, validate_data, iterations, label)\u001b[0m\n\u001b[0;32m     26\u001b[0m                     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                     \u001b[0mexample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                     nlp_train.update( [example],\n\u001b[0m\u001b[0;32m     29\u001b[0m                         \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                         \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp-gpu\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude)\u001b[0m\n\u001b[0;32m   1105\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"update\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m             \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1108\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msgd\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp-gpu\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp-gpu\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mrespect\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \"\"\"\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mInT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mOutT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp-gpu\\lib\\site-packages\\spacy\\ml\\parser_model.pyx\u001b[0m in \u001b[0;36mspacy.ml.parser_model.step_forward\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp-gpu\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    287\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\nlp-gpu\\lib\\site-packages\\thinc\\layers\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFloats2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"W\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFloats1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data =  load_data(LABEL_SPECIES, TRAIN_DATASET)\n",
    "validate_data = load_data(LABEL_SPECIES, VALIDATE_DATASET)\n",
    "nlp = train_spacy(train_data,validate_data, 20, LABEL_SPECIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "respected-dayton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.8217605248769819, 'ents_r': 0.6831818181818182, 'ents_f': 0.746090841399851, 'ents_per_type': {'SPECIES': {'p': 0.8217605248769819, 'r': 0.6831818181818182, 'f': 0.746090841399851}}, 'speed': 38213.13912196184}\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(os.path.join(NER_PATH, LABEL_SPECIES, \"best_model\"))\n",
    "test_data = load_data(LABEL_SPECIES, TEST_DATASET)\n",
    "test_data_spacy=format_data_for_evalation(test_data, nlp)\n",
    "test_metrics = nlp.evaluate(test_data_spacy)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-attachment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-sociology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#IT   | Loss       | Prec       | Recall     | F_Score    | Save Mod   \n"
     ]
    }
   ],
   "source": [
    "train_data =  load_data(LABEL_DRUG_PROTEIN, TRAIN_DATASET)\n",
    "validate_data = load_data(LABEL_DRUG_PROTEIN, VALIDATE_DATASET)\n",
    "nlp = train_spacy(train_data,validate_data, 10, LABEL_DRUG_PROTEIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(os.path.join(NER_PATH, LABEL_DRUG_PROTEIN, \"best_model\"))\n",
    "test_data = load_data(LABEL_DRUG_PROTEIN, TEST_DATASET)\n",
    "test_data_spacy=format_data_for_evalation(test_data, nlp)\n",
    "test_metrics = nlp.evaluate(test_data_spacy)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-poland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  load_data(LABEL_CHEMICAL, TRAIN_DATASET)\n",
    "validate_data = load_data(LABEL_CHEMICAL, VALIDATE_DATASET)\n",
    "nlp = train_spacy(train_data,validate_data, 10, LABEL_CHEMICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(os.path.join(NER_PATH, LABEL_CHEMICAL, \"best_model\"))\n",
    "test_data = load_data(LABEL_CHEMICAL, TEST_DATASET)\n",
    "test_data_spacy=format_data_for_evalation(test_data, nlp)\n",
    "test_metrics = nlp.evaluate(test_data_spacy)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-camel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  load_data(LABEL_DISEASE, TRAIN_DATASET)\n",
    "validate_data = load_data(LABEL_DISEASE, VALIDATE_DATASET)\n",
    "nlp = train_spacy(train_data,validate_data, 10, LABEL_DISEASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(os.path.join(NER_PATH, LABEL_DISEASE, \"best_model\"))\n",
    "test_data = load_data(LABEL_DISEASE, TEST_DATASET)\n",
    "test_data_spacy=format_data_for_evalation(test_data, nlp)\n",
    "test_metrics = nlp.evaluate(test_data_spacy)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-belgium",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
