{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "understanding-accounting",
   "metadata": {},
   "source": [
    "# Conversão dos Arquivos IOB para JSON e Spacy\n",
    "* Agrupa por Label ou por tipo de Dataset (Treino, Validação e Teste). \n",
    "* Caso deseje agrupar treino com validação (train_dev) basta ajustar o vetor DATASET_TYPE com a constante TRAIN_DEV_DATASET.\n",
    "* Junção de tokens com espaço em branco.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "further-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "operating-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_DRUG_PROTEIN = 'DRUG-PROTEIN'\n",
    "LABEL_CHEMICAL = 'CHEMICAL'\n",
    "LABEL_DISEASE = 'DISEASE'\n",
    "LABEL_SPECIES = 'SPECIES'\n",
    "\n",
    "LABEL_LIST = [LABEL_DRUG_PROTEIN,\n",
    "              LABEL_CHEMICAL,\n",
    "              LABEL_DISEASE,    \n",
    "              LABEL_SPECIES]\n",
    "\n",
    "LABEL_TO_DIR = {\n",
    "    LABEL_DRUG_PROTEIN: ['BC2GM', 'JNLPBA'],\n",
    "    LABEL_CHEMICAL: ['BC4CHEMD','BC5CDR-chem'],\n",
    "    LABEL_DISEASE: ['BC5CDR-disease', 'NCBI-disease'],    \n",
    "    LABEL_SPECIES: ['linnaeus', 's800']\n",
    "}\n",
    "\n",
    "DATA_ORIGIN_PATH = os.path.join(\"data\",\"origin\")\n",
    "DATA_PREPARED_PATH = os.path.join(\"data\", \"prepared\")\n",
    "DATA_AGGREGATE_PATH = os.path.join(DATA_PREPARED_PATH, \"aggregate\")\n",
    "MODEL_PATH = \"model\"\n",
    "MODEL_TRAIN_PATH = os.path.join(MODEL_PATH, \"prepared\")\n",
    "MODEL_ACTUAL_PATH = os.path.join(MODEL_PATH, \"actual\")\n",
    "\n",
    "TSV_EXTENSION = \".tsv\"\n",
    "JSON_EXTENSION = \".json\"\n",
    "SPACY_EXTENSION = \".spacy\"\n",
    "\n",
    "TRAIN_DEV_DATASET = \"train_dev\"\n",
    "TRAIN_DATASET = \"train\"\n",
    "VALIDATE_DATASET = \"devel\"\n",
    "TEST_DATASET = \"test\"\n",
    "\n",
    "DATASET_TYPE = [TRAIN_DATASET, VALIDATE_DATASET, TEST_DATASET]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-creation",
   "metadata": {},
   "source": [
    "### Conversão arquivo IOB para os formatos JSON(formato próprio) e SPACY (para treinamento em linha de comando)\n",
    "#### Json gerado com formato próprio que será tratado no momento do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acting-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processa_iob_dataset(lista_annotations, label, dataset_type, dir_dataset):\n",
    "    entidade_atual = \"\"\n",
    "    ini_entidade_atual = -1\n",
    "    entities = []\n",
    "    sentenca = \"\"\n",
    "    \n",
    "    dataset_iob_file = os.path.join(DATA_ORIGIN_PATH, dir_dataset, dataset_type + TSV_EXTENSION)\n",
    "    with open(dataset_iob_file) as f_iob:\n",
    "        for linha in f_iob:\n",
    "            if len(entidade_atual) > 0 and (\"\\tO\" in linha or \"\\tB\" in linha or linha == \"\\n\"):\n",
    "                entities.append({\"entidade\":entidade_atual, \n",
    "                                     \"start\":ini_entity_atual, \n",
    "                                     \"end\": ini_entity_atual + len(entidade_atual),\n",
    "                                     \"label\": label\n",
    "                                    })\n",
    "                entidade_atual = \"\"\n",
    "                ini_entity_atual = -1\n",
    "\n",
    "            if linha != \"\\n\":\n",
    "                if (len(sentenca) != 0):\n",
    "                    sentenca += \" \"\n",
    "                if len(entidade_atual) > 0:\n",
    "                    entidade_atual += \" \"\n",
    "\n",
    "                if (\"\\tO\" in linha):\n",
    "                    linha_tratada = linha.replace(\"\\tO\",\"\").replace(\"\\n\", \"\")                            \n",
    "                elif(\"\\tB\" in linha):\n",
    "                    ini_entity_atual = len(sentenca)\n",
    "                    linha_tratada = linha.replace(\"\\tB\",\"\").replace(\"\\n\", \"\")\n",
    "                    entidade_atual = linha_tratada                \n",
    "                elif(\"\\tI\" in linha):\n",
    "                    linha_tratada = linha.replace(\"\\tI\",\"\").replace(\"\\n\", \"\")\n",
    "                    entidade_atual += linha_tratada\n",
    "\n",
    "                sentenca = sentenca + linha_tratada\n",
    "            else:\n",
    "                lista_annotations.append({\"texto\": sentenca, \"entities\": entities})\n",
    "                sentenca = \"\"\n",
    "                entities=[]\n",
    "    return lista_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "smooth-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path - caminho que será gravado, sem o nome do arquivo\n",
    "# file_name - nome do arquivo sem extensão\n",
    "def save_converted_file(lista_annotations, path, file_name, save_json, save_spacy):\n",
    "    if save_json:\n",
    "        json_file = os.path.join(path, file_name + JSON_EXTENSION)\n",
    "        if os.path.exists(json_file):\n",
    "            os.remove(json_file)\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        with open(json_file, 'w') as json_file:            \n",
    "            json.dump(lista_annotations, json_file)\n",
    "\n",
    "    if save_spacy:\n",
    "        nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "        db = DocBin() # create a DocBin object\n",
    "        for an in lista_annotations:\n",
    "            doc = nlp.make_doc(an['texto']) # create doc object from text\n",
    "            ents=[]\n",
    "            for entidade in an['entities']:\n",
    "                span = doc.char_span(entidade['start'], entidade['end'], label=entidade['label'], alignment_mode=\"contract\")\n",
    "                if span is None:\n",
    "                    print (\"Span None\")\n",
    "                    print(ner['texto'])\n",
    "                    print(entidade)\n",
    "                else:\n",
    "                    ents.append(span)\n",
    "\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "\n",
    "        spacy_file = os.path.join(path, file_name + SPACY_EXTENSION)\n",
    "        if os.path.exists(spacy_file):\n",
    "            os.remove(spacy_file)\n",
    "        db.to_disk(spacy_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "thousand-messenger",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_IOB_json_spacy(conv_json=True, conv_spacy=True, group_by_label=True, group_by_dataset_type=False):\n",
    "    \n",
    "    if group_by_dataset_type and group_by_label:\n",
    "        raise Exception(\"Agrupamento deve ser por label ou por dataset, são mutuamente exclusivos\")\n",
    "    \n",
    "    for dataset_type in DATASET_TYPE:\n",
    "        if group_by_dataset_type:\n",
    "            lista_annotations = []\n",
    "        for label in LABEL_LIST:\n",
    "            if (group_by_label):\n",
    "                lista_annotations = []\n",
    "    \n",
    "            for dir_dataset in LABEL_TO_DIR[label]:\n",
    "                lista_annotations = processa_iob_dataset(lista_annotations, label, dataset_type, dir_dataset)\n",
    "\n",
    "            if group_by_label:\n",
    "                path = os.path.join(DATA_PREPARED_PATH, label)\n",
    "                file = label + \"-\" + dataset_type\n",
    "                save_converted_file(lista_annotations, path, file, save_json=conv_json, save_spacy=conv_spacy)\n",
    "        if group_by_dataset_type:\n",
    "            file = dataset_type\n",
    "            save_converted_file(lista_annotations, DATA_AGGREGATE_PATH, file, save_json=conv_json, save_spacy=conv_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-chess",
   "metadata": {},
   "source": [
    "#### Converte IOB para JSON agrupando por Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_IOB_json_spacy(conv_json=True, conv_spacy=False, group_by_label=False, group_by_dataset_type=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-preliminary",
   "metadata": {},
   "source": [
    "#### Converte IOB para JSON agrupando por Dataset (Train, Dev e Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "restricted-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_IOB_json_spacy(conv_json=True, conv_spacy=False, group_by_label=False, group_by_dataset_type=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-portland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
