{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polish-arabic",
   "metadata": {},
   "source": [
    "# Exemplo de Treinos com Spacy para Reconhecimento de Entidades (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-refrigerator",
   "metadata": {},
   "source": [
    "#### O exemplo foi montado a partir do código Kaggle informado pelo professor e também por um exemplo do Medium, seguem os links:\n",
    "* https://www.kaggle.com/finalepoch/medical-ner-using-spacy/notebook\n",
    "* https://towardsdatascience.com/train-ner-with-custom-training-data-using-spacy-525ce748fab7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chief-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "#import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-musical",
   "metadata": {},
   "source": [
    "### Modelo Puro Spacy com exemplos de entitades não reconhecidas ou identificadas incorretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "direct-jacob",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who PRON\n",
      "is AUX\n",
      "Nishant ADJ\n",
      "? PUNCT\n",
      "He PRON\n",
      "is AUX\n",
      "in ADP\n",
      "Minas PROPN\n",
      "Gerais PROPN\n",
      "or CCONJ\n",
      "Macaé PROPN\n",
      ", PUNCT\n",
      "Brazil PROPN\n",
      "or CCONJ\n",
      "Brasil PROPN\n",
      ". PUNCT\n",
      "I PRON\n",
      "am AUX\n",
      "in ADP\n",
      "my PRON\n",
      "apartment NOUN\n",
      "at ADP\n",
      "Barra PROPN\n",
      "da PROPN\n",
      "Tijuca PROPN\n",
      ". PUNCT\n",
      "If SCONJ\n",
      "Anderson PROPN\n",
      ", PUNCT\n",
      "Gustavo PROPN\n",
      "and CCONJ\n",
      "Astolfo PROPN\n",
      "met VERB\n",
      "up ADP\n",
      "us PRON\n",
      ", PUNCT\n",
      "we PRON\n",
      "will AUX\n",
      "make VERB\n",
      "a DET\n",
      "party NOUN\n",
      ". PUNCT\n",
      "--- ENT ---\n",
      "Minas Gerais 25 37 ORG\n",
      "Macaé 41 46 GPE\n",
      "Brazil 48 54 GPE\n",
      "Brasil 58 64 PERSON\n",
      "Barra da Tijuca 90 105 ORG\n",
      "Anderson 110 118 PERSON\n",
      "Astolfo 132 139 GPE\n"
     ]
    }
   ],
   "source": [
    "nlp_pre_treinado_sm = spacy.load('en_core_web_sm')\n",
    "doc = nlp_pre_treinado_sm(u\"Who is Nishant? He is in Minas Gerais or Macaé, Brazil or Brasil. I am in my apartment at Barra da Tijuca. If Anderson, Gustavo and Astolfo met up us, we will make a party.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "print('--- ENT ---')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.start_char, ent.end_char,ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "purple-carbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kamal Khumar 7 19 PERSON\n",
      "Maria 44 49 PERSON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_pre_treinado_sm(u\"Who is Kamal Khumar? Who is Nishant? Who is Maria? \")\n",
    "for token in doc.ents:\n",
    "    print(token.text,token.start_char, token.end_char,token.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-engine",
   "metadata": {},
   "source": [
    "#### Pequena base de treino para reconhecimento de Entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "naked-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    ('Where is Barra da Tijuca?', {\n",
    "        'entities': [(9, 24, 'LOC')]\n",
    "    }),\n",
    "    ('Who is Astolfo?', {\n",
    "        'entities': [(7, 14, 'PERSON')]\n",
    "    }),\n",
    "     ('Who is Gustavo?', {\n",
    "        'entities': [(7, 14, 'PERSON')]\n",
    "    }),\n",
    "    ('I like London and Berlin.', {\n",
    "        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "    })\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-request",
   "metadata": {},
   "source": [
    "### Para treinar um modelo do Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hybrid-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_blank = spacy.blank('en')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-steps",
   "metadata": {},
   "source": [
    "##### No Spacy 3 não é mais necessário fazer um create_pipe (os exemplos utilzam create_pipe e add_pipe posteriormente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "visible-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ner' not in nlp_blank.pipe_names:\n",
    "    ner_blank = nlp_blank.add_pipe('ner')    \n",
    "else:\n",
    "    ner_blank = nlp_blank.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-worth",
   "metadata": {},
   "source": [
    "##### Aqui também tivemos mudanças no Spacy 3 - Quando iterar pelo batch é necessário criar objetos *Example*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ranking-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 17.48537713289261}\n",
      "{'ner': 15.800669848918915}\n",
      "{'ner': 12.777700930833817}\n",
      "{'ner': 10.85413184762001}\n",
      "{'ner': 8.27435595728457}\n",
      "{'ner': 7.547029630979523}\n",
      "{'ner': 6.824094576993957}\n",
      "{'ner': 6.978209424414672}\n",
      "{'ner': 5.9165332875272725}\n",
      "{'ner': 5.373996594109485}\n",
      "{'ner': 6.201074225849879}\n",
      "{'ner': 4.175207329715789}\n",
      "{'ner': 4.278953146247204}\n",
      "{'ner': 4.564560299518924}\n",
      "{'ner': 4.764484313191133}\n",
      "{'ner': 3.4053405030980457}\n",
      "{'ner': 3.2561352893029767}\n",
      "{'ner': 3.802131842158701}\n",
      "{'ner': 3.1804286987965034}\n",
      "{'ner': 3.173197740411191}\n",
      "{'ner': 3.024884076604}\n",
      "{'ner': 2.039549062062684}\n",
      "{'ner': 2.8410471055105115}\n",
      "{'ner': 2.334606500805936}\n",
      "{'ner': 6.607993241588902}\n",
      "{'ner': 1.686455891403225}\n",
      "{'ner': 0.9864755109491186}\n",
      "{'ner': 1.6112177401109133}\n",
      "{'ner': 1.8702583382498312}\n",
      "{'ner': 3.3655961177128453}\n",
      "{'ner': 3.234964168316506}\n",
      "{'ner': 1.9997512892387137}\n",
      "{'ner': 0.6362937198690592}\n",
      "{'ner': 3.5437545845396983}\n",
      "{'ner': 1.8201272295806414}\n",
      "{'ner': 0.9437481277211759}\n",
      "{'ner': 1.4116392646645446}\n",
      "{'ner': 1.9215829227596772}\n",
      "{'ner': 2.3819282394257333}\n",
      "{'ner': 1.0507085516585826}\n",
      "{'ner': 1.7760914132749848}\n",
      "{'ner': 0.17697922211339084}\n",
      "{'ner': 0.6814816767586965}\n",
      "{'ner': 0.23826403103213367}\n",
      "{'ner': 0.09927976489157388}\n",
      "{'ner': 0.1877705701306054}\n",
      "{'ner': 0.12999952929252537}\n",
      "{'ner': 0.00418190917802237}\n",
      "{'ner': 0.06388435875955746}\n",
      "{'ner': 0.013820505174428337}\n",
      "{'ner': 0.0033607538315193084}\n",
      "{'ner': 0.1621556631896867}\n",
      "{'ner': 0.034861822908211236}\n",
      "{'ner': 0.015473175607360435}\n",
      "{'ner': 0.00021906039025014907}\n",
      "{'ner': 0.0005220224017590374}\n",
      "{'ner': 4.775395963501115e-05}\n",
      "{'ner': 0.011360391215797294}\n",
      "{'ner': 4.210275177738413e-07}\n",
      "{'ner': 0.010810143781197459}\n",
      "{'ner': 1.0684063447677401e-07}\n",
      "{'ner': 4.061949799421584e-06}\n",
      "{'ner': 8.177951128056624e-05}\n",
      "{'ner': 1.4568825893151771}\n",
      "{'ner': 0.036603326911584216}\n",
      "{'ner': 0.008787947664785108}\n",
      "{'ner': 0.00012644398053235302}\n",
      "{'ner': 0.010336730906937822}\n",
      "{'ner': 5.316540199672103e-07}\n",
      "{'ner': 0.6171336595590634}\n",
      "{'ner': 0.0002144170906697691}\n",
      "{'ner': 1.9745796072380466}\n",
      "{'ner': 6.099957631514762e-07}\n",
      "{'ner': 3.2419488905753924e-05}\n",
      "{'ner': 1.1363739039120972e-07}\n",
      "{'ner': 0.0001141547973343376}\n",
      "{'ner': 3.117441275021195e-06}\n",
      "{'ner': 1.5136064606018696e-06}\n",
      "{'ner': 2.5989357313774158e-05}\n",
      "{'ner': 1.1938662545601953e-06}\n",
      "{'ner': 0.00020622371047229724}\n",
      "{'ner': 0.0049707352412133015}\n",
      "{'ner': 0.0018715939399481624}\n",
      "{'ner': 5.8570733597841313e-08}\n",
      "{'ner': 1.5979964869099454e-10}\n",
      "{'ner': 1.4903236093274173e-06}\n",
      "{'ner': 4.960253964670378e-06}\n",
      "{'ner': 2.4657557823729774e-07}\n",
      "{'ner': 1.880754292484124e-08}\n",
      "{'ner': 3.428235999975365e-06}\n",
      "{'ner': 1.2767882006658986e-05}\n",
      "{'ner': 0.0016398676713194468}\n",
      "{'ner': 8.918512026767831e-07}\n",
      "{'ner': 0.0018157368393709095}\n",
      "{'ner': 1.4127186373516694e-09}\n",
      "{'ner': 4.0295433264883987e-07}\n",
      "{'ner': 1.0376175574648206e-07}\n",
      "{'ner': 0.0316811985684522}\n",
      "{'ner': 7.738049409735478e-08}\n",
      "{'ner': 1.4621524395556214e-07}\n"
     ]
    }
   ],
   "source": [
    "n_iter=100\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner_blank.add_label(ent[2])\n",
    "\n",
    "#usando os mesmos pipes do exemplo Kaggle\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp_blank.pipe_names if pipe not in pipe_exceptions]\n",
    "with nlp_blank.disable_pipes(*other_pipes):  \n",
    "    optimizer = nlp_blank.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        batches = minibatch(TRAIN_DATA, size=2)\n",
    "        for batch in batches:\n",
    "            for text, annotations in batch:\n",
    "                doc = nlp_blank.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp_blank.update( [example],\n",
    "                    drop=0.5,  \n",
    "                    sgd=optimizer,\n",
    "                    losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-criminal",
   "metadata": {},
   "source": [
    "#### Resultado do treino a partir do zero é muito ruim, embora esperado, nossa base de treino é inadequada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stylish-fundamentals",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who \n",
      "is \n",
      "Nishant \n",
      "? \n",
      "He \n",
      "is \n",
      "in \n",
      "Minas \n",
      "Gerais \n",
      "or \n",
      "Macaé \n",
      ", \n",
      "Brazil \n",
      "or \n",
      "Brasil \n",
      ". \n",
      "I \n",
      "am \n",
      "in \n",
      "my \n",
      "apartment \n",
      "at \n",
      "Barra \n",
      "da \n",
      "Tijuca \n",
      ". \n",
      "If \n",
      "Anderson \n",
      ", \n",
      "Gustavo \n",
      "and \n",
      "Astolfo \n",
      "met \n",
      "up \n",
      "us \n",
      ", \n",
      "we \n",
      "will \n",
      "make \n",
      "a \n",
      "party \n",
      ". \n",
      "--- ENT ---\n",
      "Nishant 7 14 PERSON\n",
      "Minas 25 30 PERSON\n",
      "Gerais 31 37 PERSON\n",
      "Macaé 41 46 LOC\n",
      "Brazil 48 54 LOC\n",
      "Brasil 58 64 LOC\n",
      "Barra da Tijuca 90 105 LOC\n",
      "Anderson 110 118 PERSON\n",
      "Gustavo 120 127 PERSON\n",
      "Astolfo 132 139 LOC\n",
      ", we will 149 158 LOC\n",
      "party 166 171 LOC\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_blank(u\"Who is Nishant? He is in Minas Gerais or Macaé, Brazil or Brasil. I am in my apartment at Barra da Tijuca. If Anderson, Gustavo and Astolfo met up us, we will make a party.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "print('--- ENT ---')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.start_char, ent.end_char,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-bermuda",
   "metadata": {},
   "source": [
    "## Reaproveitando o modelo Pré-Treinado Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sunset-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_comp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-diameter",
   "metadata": {},
   "source": [
    "##### No Spacy 3 não é mais necessário fazer um create_pipe (os exemplos utilzam create_pipe e add_pipe posteriormente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "celtic-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ner' not in nlp_comp.pipe_names:\n",
    "    ner_comp = nlp_comp.add_pipe('ner')    \n",
    "else:\n",
    "    ner_comp = nlp_comp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-benjamin",
   "metadata": {},
   "source": [
    "##### Para utilizar um modelo pré-treinado do Spacy é necessário utilizar *create_optimizer* ao invés de begin_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "incorrect-contract",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 11.165581656949426}\n",
      "{'ner': 5.177444001609533}\n",
      "{'ner': 5.435967956115739}\n",
      "{'ner': 6.862212636792849}\n",
      "{'ner': 5.361327985616445}\n",
      "{'ner': 3.6201770044390473}\n",
      "{'ner': 7.487692069636954}\n",
      "{'ner': 2.6575723538952607}\n",
      "{'ner': 2.923849790352127}\n",
      "{'ner': 0.31361643224690994}\n",
      "{'ner': 1.9999626883582804}\n",
      "{'ner': 0.09339892612066049}\n",
      "{'ner': 0.006701308884077068}\n",
      "{'ner': 0.0029198915784581963}\n",
      "{'ner': 4.493339208968703e-05}\n",
      "{'ner': 0.004537594570875606}\n",
      "{'ner': 0.0005037831261631808}\n",
      "{'ner': 0.00020129984464048928}\n",
      "{'ner': 0.2154272784489902}\n",
      "{'ner': 6.635966388707149e-05}\n",
      "{'ner': 0.03085110000431699}\n",
      "{'ner': 0.12313657447292987}\n",
      "{'ner': 4.8241546098040704e-05}\n",
      "{'ner': 1.9482305500996857}\n",
      "{'ner': 7.567607187445672e-05}\n",
      "{'ner': 1.3311514316339505e-05}\n",
      "{'ner': 0.3379472472738589}\n",
      "{'ner': 2.0597767749759278e-05}\n",
      "{'ner': 0.0195511118569808}\n",
      "{'ner': 2.0958307901139746e-06}\n",
      "{'ner': 3.5163345704016714e-06}\n",
      "{'ner': 0.0016660147394312913}\n",
      "{'ner': 0.00038475593740161187}\n",
      "{'ner': 2.887879041947466e-07}\n",
      "{'ner': 5.182082387251214e-09}\n",
      "{'ner': 0.0001736008824324221}\n",
      "{'ner': 0.10950537460596593}\n",
      "{'ner': 5.383327443378198e-07}\n",
      "{'ner': 2.2235205999995745e-07}\n",
      "{'ner': 0.2570152063361392}\n",
      "{'ner': 0.013045735405608539}\n",
      "{'ner': 0.014266288102997262}\n",
      "{'ner': 3.002683235382288e-08}\n",
      "{'ner': 0.003035473366193732}\n",
      "{'ner': 1.2817015512296352e-06}\n",
      "{'ner': 8.034452666654502e-06}\n",
      "{'ner': 1.9498787355324266e-08}\n",
      "{'ner': 0.002627790445254472}\n",
      "{'ner': 3.3225934744490892e-06}\n",
      "{'ner': 2.803276388817362e-09}\n",
      "{'ner': 1.538553720761888e-10}\n",
      "{'ner': 4.039577014535752e-06}\n",
      "{'ner': 1.3782271131463775e-05}\n",
      "{'ner': 0.00029618972681641286}\n",
      "{'ner': 4.3836020827272604e-05}\n",
      "{'ner': 1.5658470044409372e-05}\n",
      "{'ner': 2.8046740489495397e-07}\n",
      "{'ner': 4.1953717572014607e-07}\n",
      "{'ner': 2.6200208544567505e-07}\n",
      "{'ner': 0.0005571802702743188}\n",
      "{'ner': 0.005524561145743225}\n",
      "{'ner': 7.753379933838901e-08}\n",
      "{'ner': 0.0008571709988683833}\n",
      "{'ner': 2.4619117629113397e-09}\n",
      "{'ner': 0.16147009521002173}\n",
      "{'ner': 8.51167142606365e-06}\n",
      "{'ner': 4.912788543337668e-09}\n",
      "{'ner': 1.0487061809491848e-08}\n",
      "{'ner': 1.6144888667463866e-09}\n",
      "{'ner': 3.1890933345603483e-12}\n",
      "{'ner': 6.118454452845518e-11}\n",
      "{'ner': 9.835627755728708e-07}\n",
      "{'ner': 3.3440247733900005e-09}\n",
      "{'ner': 3.830771603147642e-08}\n",
      "{'ner': 2.9268154747464644e-06}\n",
      "{'ner': 4.88668226562012e-09}\n",
      "{'ner': 4.5065268839628175e-06}\n",
      "{'ner': 1.279032643353638e-09}\n",
      "{'ner': 8.667406809024519e-11}\n",
      "{'ner': 4.3221569803379365e-07}\n",
      "{'ner': 6.76258057656819e-07}\n",
      "{'ner': 1.217194926201388e-07}\n",
      "{'ner': 0.00011695322295681511}\n",
      "{'ner': 0.0004461015437914592}\n",
      "{'ner': 5.57252286375957e-07}\n",
      "{'ner': 3.421625747699217e-09}\n",
      "{'ner': 2.807333176384346e-10}\n",
      "{'ner': 1.5631759210319576e-10}\n",
      "{'ner': 2.0959579824196904e-07}\n",
      "{'ner': 7.582279295877883e-12}\n",
      "{'ner': 2.101012222797043e-10}\n",
      "{'ner': 3.089301735796585e-10}\n",
      "{'ner': 0.046767205209426364}\n",
      "{'ner': 5.187496613450091e-08}\n",
      "{'ner': 3.0101734400943563e-09}\n",
      "{'ner': 3.568582782566191e-10}\n",
      "{'ner': 3.2687965772068844e-08}\n",
      "{'ner': 1.895888415630173e-09}\n",
      "{'ner': 2.180259455959458e-09}\n",
      "{'ner': 2.553058943052216e-09}\n"
     ]
    }
   ],
   "source": [
    "n_iter=100\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner_comp.add_label(ent[2])\n",
    "\n",
    "#usando os mesmos pipes do exemplo Kaggle\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp_comp.pipe_names if pipe not in pipe_exceptions]\n",
    "with nlp_comp.disable_pipes(*other_pipes):  \n",
    "    optimizer = nlp_comp.create_optimizer()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        batches = minibatch(TRAIN_DATA, size=2)\n",
    "        for batch in batches:\n",
    "            for text, annotations in batch:\n",
    "                doc = nlp_comp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp_comp.update( [example],\n",
    "                    drop=0.5,  \n",
    "                    sgd=optimizer,\n",
    "                    losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-nelson",
   "metadata": {},
   "source": [
    "#### Resultado muito melhor do que o teste anterior, conforme esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "charming-bottle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who PRON\n",
      "is AUX\n",
      "Nishant ADJ\n",
      "? PUNCT\n",
      "He PRON\n",
      "is AUX\n",
      "in ADP\n",
      "Minas PROPN\n",
      "Gerais PROPN\n",
      "or CCONJ\n",
      "Macaé PROPN\n",
      ", PUNCT\n",
      "Brazil PROPN\n",
      "or CCONJ\n",
      "Brasil PROPN\n",
      ". PUNCT\n",
      "I PRON\n",
      "am AUX\n",
      "in ADP\n",
      "my PRON\n",
      "apartment NOUN\n",
      "at ADP\n",
      "Barra PROPN\n",
      "da PROPN\n",
      "Tijuca PROPN\n",
      ". PUNCT\n",
      "If SCONJ\n",
      "Anderson PROPN\n",
      ", PUNCT\n",
      "Gustavo PROPN\n",
      "and CCONJ\n",
      "Astolfo PROPN\n",
      "met VERB\n",
      "up ADP\n",
      "us PRON\n",
      ", PUNCT\n",
      "we PRON\n",
      "will AUX\n",
      "make VERB\n",
      "a DET\n",
      "party NOUN\n",
      ". PUNCT\n",
      "--- ENT ---\n",
      "Nishant 7 14 PERSON\n",
      "Minas Gerais 25 37 LOC\n",
      "Macaé 41 46 LOC\n",
      "Brazil 48 54 LOC\n",
      "Brasil 58 64 PERSON\n",
      "Barra da Tijuca 90 105 LOC\n",
      "Anderson 110 118 PERSON\n",
      "Gustavo 120 127 PERSON\n",
      "Astolfo 132 139 PERSON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_comp(u\"Who is Nishant? He is in Minas Gerais or Macaé, Brazil or Brasil. I am in my apartment at Barra da Tijuca. If Anderson, Gustavo and Astolfo met up us, we will make a party.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "print('--- ENT ---')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.start_char, ent.end_char,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-delay",
   "metadata": {},
   "source": [
    "#### Salvando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "refined-zealand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to C:\\temporario\n"
     ]
    }
   ],
   "source": [
    "output_dir=Path(\"C:\\\\temporario\")\n",
    "output_dir = Path(output_dir)\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "nlp_comp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-rally",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
